\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{geometry}

% Set margins
\geometry{margin=1in}
\documentclass{article}
\usepackage{amsmath}
\usepackage{hyperref}

\title{Artificial Intelligence Through Time: A Brief Historical Review}
\author{
    Dilli Hang Rai \\
    \texttt{dillihangrai.078@godawari.edu.np} \\
    \texttt{dillihangrae@gmail.com} \\
    Bachelor of Science in Computer Science and Information Technology \\
    Tribhuvan University, Institute of Science and Technology \\
    ORCID: \texttt{0009-0007-2200-4351}
}

\date{\today}



\begin{document}

\maketitle

\begin{abstract}
{

The world is experiencing a revolutionary transformation in artificial intelligence and technology, largely due to the rise of LLMs (Large Language Models), which are deep neural networks trained on vast datasets with billions of parameters, enabling them to generate and understand human language. Rolling back and understanding the history of AI is essential for gaining theoretical knowledge and insights into its development. This review paper offers a detailed, straightforward, and comprehensive overview of the timeline of AI history, highlighting significant milestones from Aristotle to the present. It aims to compile and summarize essential information about artificial intelligence (AI) by citing relevant sources and presenting a clear timeline, along with a brief analysis of significant resources that contribute to an understanding of the evolution of AI.

This review traces the evolution of artificial intelligence from its philosophical roots through key milestones like the Dartmouth Conference, early successes, Expert Systems, AI winters, and recent breakthroughs in machine learning and deep learning, highlighting the field's cyclical nature of enthusiasm and setbacks, its ongoing pursuit of human-level intelligence amid technological advancements, and ethical considerations. Designed for readers with varying levels of familiarity with AI, it offers a comprehensive understanding of its development in one accessible location. Furthermore, the paper aims to help readers compare modern and classical AI, enabling them to make informed predictions about future developments.

}
\subsection*{Keywords:} Artificial Intelligence (AI),
Alan Turing,
Neural Networks,
Claude Shannon,
John McCarthy,
Symbolic Systems,
Propositional Logic,
Herbert Simon,
Game Development,
Machine Translation

\section{Introduction}

Today will become tomorrow’s history. The effort to create truly intelligent machines
has significantly shaped the field of Artificial Intelligence (AI) and laid the groundwork for its development over the years. The history of AI has evolved from early
philosophical ideas to today’s advanced computational systems.
AI has been developing since the 1940s, experiencing various ups and downs, including periods known as ”AI Winters.” Now, it has grown into a thriving billion-dollar industry. This review paper looks at the historical progress of
AI and traces its roots from early theories to key figures who have contributed to the Field, important innovations, and
foundational concepts like symbolic logic. It also examines the rise of neural networks
and machine learning.
Understanding this history provides valuable insights into the impact of AI and
sheds light on both the challenges and opportunities.

\section{Ancient Historical \& Philosophical Context}

AI's origins date back to Aristotle’s (384-322 B.C.) deductive reasoning with syllogisms. Ramon Llull's (ca. 1235-1316) Ars Magna, a machine meant to answer all questions.
Earlier thinkers such as Aristotle, Leibniz, Babbage, and Hollerith laid the groundwork for AI, influencing its theoretical and practical developments.

For Aristotle, change was a fascinating aspect of nature. In "Physics," he described his "philosophy of nature" as the study of things that transform, distinguishing between matter and form. For instance, a sculpture is made of bronze (matter) and shaped into a human figure (form). 

This matter/form distinction underpins modern ideas like symbolic computing and data abstraction. In computing, we manipulate patterns that represent the forms of electromagnetic material, with changes in these forms reflecting the solution process. By abstracting the form from its medium, we enhance the manipulation of data structures, which is central to computer science and the development of artificial intelligence.

In his *Metaphysics*, Aristotle begins with “All men by nature desire to know” and develops a science of unchanging things, including cosmology and theology. More pertinent to artificial intelligence is his epistemology in *Logic*, where he analyzes how humans understand their world. He termed logic the “instrument” (organon), believing it underpins all knowledge.

In *Logic*, Aristotle examined whether certain propositions can be deemed “true” based on other known truths. For example, if “all men are mortal” and “Socrates is a man,” we can conclude “Socrates is mortal.” This reasoning illustrates his concept of syllogism, specifically modus ponens.


\section{Before 1940s}
The Pascaline, created by French philosopher and mathematician Blaise Pascal in 1642, is another notable calculating machine. Although Schickard and Pascal's devices could only perform addition and subtraction, they proved that processes once believed to need human skill could be automated.

George Boole (Boole, 1854) pioneered propositional logic, significantly advancing our understanding of human cognition and laying the groundwork for modern logic.
In the late 19th century, Gottlieb Frege developed a notational system for mechanical reasoning, which he called Begriffsschrift, meaning "concept writing," leading to the creation of predicate calculus [Frege 1879].
Pascal’s work on calculating machines inspired Leibniz to develop the Leibniz Wheel in 1694, a machine capable of multiplication and division using a hand crank and moveable carriage. Fascinated by automated logic, Leibniz proposed a machine that could calculate logical conclusions based on the necessary and sufficient features of concepts. In 1887, he envisioned a device for automated deductive reasoning and scientific knowledge production, foreshadowing modern ideas of inference and proof automation. 


\section{Incubation Period (1940 - 1950)}
This era can be called an incubation period of AI, as suggested by Russell in his book, where the AI field itself emerged with digital computers, and computers became commercially available.
The Turing Test, proposed by Alan Turing in 1950, is a test of a machine's ability to exhibit intelligent behavior indistinguishable from
 that of a human. A human evaluator communicates with both a human and a machine via text-only interface, and if the evaluator cannot reliably distinguish between the two, the machine is said to have passed the test.
 
\subsection{Influence of Cybernetics and Cognitive Psychology(1940s)}
\begin{itemize}
    \item \textbf{Prototypes of Digital Computers:} Early computer systems, such as the Mark I relay computer and the ENIAC, were developed.
    \item \textbf{Cybernetics Emerges:} Norbert Wiener coined the term ``cybernetics,'' studying communication between humans and machines, integrating information theory, feedback control systems, and electronic computing and cognitive psychology as a field. This era addressed tasks like symbolic integration and mobile robot control, but many systems could only solve simple ``toy problems.''
\end{itemize}

\subsection{Perceptron Learning: McCulloch \& Pitts (1943)}
\begin{itemize}
    \item Theorized connections between computing elements and biological neurons.
    \item Demonstrated that networks of logical gates could represent any computable function.
\end{itemize}

\subsection{Commerical Reality (1944)}
The Mark I Harvard relay computer, a prototype system, demonstrated the commercial viability of electronic stored-program digital computers.

\subsection{Vannevar Bush's Vision(1945)}
Vannevar Bush published "As We May Think," envisioning a future where computers assist humans in various tasks. It offers a visionary perspective on the potential of technology to augment human thought and memory.

\subsection{ENIAC (1947)}
The ENIAC, a more advanced electronic computer developed at the University of
Pennsylvania further pushed the boundaries of computer technology and marked a
a significant step forward in the evolution of digital computing.

\subsection{
Intelligent Machinery (1948)}
Alan Turing, one of the key figures in early AI, wrote "Intelligent Machinery", where he explored ideas about machine learning and the potential of machines to simulate human intelligence.

\subsection{Learning Networks (1949)}
McCulloch and Pitts suggested that networks could learn. Donald Hebb introduced Hebbian learning in 1949, a fundamental rule for modifying neuron connection strengths, which remains influential today.

\subsection{Alan Turing's Contributions (1947 \& 1950)}
Gave lectures as early as 1947 and wrote “Computing Machinery and Intelligence” in 1950. Introduced the Turing Test, machine learning, genetic algorithms, and reinforcement learning. Proposed the "Child Programme" concept for simulating child-like learning. The first significant exploration of mechanizing human intelligence came from Alan Turing.

\subsection{Claude Shannon's Work (1950)}
Programming a Computer for Playing Chess. Developed programs for simple reasoning tasks, advancing the exploration of mechanizing human intelligence.

\subsection{First Neural Network Computer (1950-1951)}
Marvin Minsky and Dean Edmonds built the SNARC at Harvard, using 3,000 vacuum tubes to simulate a network of 40 neurons. Later, Minsky studied universal computation in neural networks at Princeton.

\subsection{Advancements in Formal Grammar (1950)}
In the 1950s, Noam Chomsky’s Generative Grammar and developments in formal grammar from logic provides new insights into linguistics and language theory.

% -------=================================-------=================================
% -------=================================-------=================================
% -------=================================-------=================================
\section{Birth of AI (1950 - 1960)}
The 1950s were pivotal in the establishment of AI, with researchers such as Claude Shannon at MIT and Allen Newell at the RAND Corporation creating chess-playing and simulation programs. Their pioneering efforts set the stage for future advancements in AI. In the late 1950s, the focus shifted to significant developments in pattern recognition and self-adapting systems. 

\subsection{Alan Turing's "Imitation Game" and Turing Test(1950)}
Alan Turing's "Computing Machinery and Intelligence": In 1950, Alan Turing published his seminal paper where he proposed the Turing Test (originally called the "Imitation Game"). This paper raised fundamental questions about whether machines could think and introduced the idea of measuring machine intelligence by comparing a machine's ability to imitate human responses in a conversation. This concept remains one of the foundational ideas in AI.

\subsection{Emerging AI Programs(1951)}
The first working AI programs were developed for the Ferranti Mark 1 computer at the University of Manchester. Christopher Strachey created a checkers-playing program, while Dietrich Prinz developed a chess-playing program

\subsection{Game Development(1952)}
\textbf{Arthur Samuel's Checkers Program:} Samuel begins developing programs for the Checkers game that can learn to play at a strong amateur level, demonstrating that computers can learn beyond explicit instructions. \\
\textbf{Claude Shannon's Chess Programs:} Shannon begins developing chess-playing programs at MIT, contributing to early AI research in game-playing algorithms. 

\subsection{Georgetown-IBM Experiment(1953))}
Georgetown-IBM Experiment was the first successful machine translation of 250 sentences from English to Russian using the IBM 701 mainframe, generating significant public interest and funding for future research.

\subsection{FORTRAN Development Begins(1954)}
\textbf{} The development of FORTRAN, one of the first high-level programming languages, starts, facilitating future AI programming.

\subsection{ Proposal for the Dartmouth workshop (1955)}
\textbf{} In 1955, John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon proposed a summer workshop at Dartmouth College in 1956, which is now considered the founding event of AI as a field.

\subsection{Noam Chomsky's Generative Grammar(1955-1957)}
\textbf Chomsky’s work on generative grammar greatly influences computational linguistics and natural language processing. He published the book “Syntactic Structures” in 1957.

\subsection{IPL(1956 - 1957)}
Newell, Shaw, and Simon completed the Logic Theorist, one of the earliest programs for automatic theorem proving. This development also saw the creation of IPL (Information Processing Language), the first list-processing language.


\subsection{Birthplace of AI (1956)}
McCarthy enlisted Minsky, Claude Shannon, and Nathaniel Rochester to gather U.S. researchers focused on automata theory, neural networks, and intelligence studies. 

\textbf{February (1956):} 
Television Demonstration: Samuel's Checker's program is showcased on television, impressing audiences with its learning capabilities. 
\textbf{Summer (1956):} 
\begin{itemize}
    \item \textbf{Dartmouth Workshop:} John McCarthy organizes a workshop at Dartmouth, gathering key figures like Minsky, Claude Shannon, and Nathaniel Rochester to discuss automata theory, neural networks, and intelligence. This event is recognized as the official birth of AI.
\end{itemize}

\textbf{Automata Studies (1956):} 
\begin{itemize}
    \item \textbf{} John McCarthy and Claude Shannon edited a volume that lacked a strong focus on AI.
\end{itemize}

\subsection{Machine Translation Efforts (1957)}
\begin{itemize}
    \item \textbf{Herbert Simon's Statement:} Simon claims that machines now think, learn, and create, and predicts rapid advancements in their capabilities to match human cognitive abilities.
    \item \textbf{Machine Translation Efforts:} The U.S. National Research Council funds projects to expedite the translation of Russian scientific papers, revealing challenges in early machine translation, exemplified by humorous errors.
\end{itemize}

\subsection{Chess, Advice Taker, and LISP Developments (1958)}
\begin{itemize}
    \item \textbf{Newell, Shaw \& Simon's Chess Program:} Notable developments in AI with chess-playing programs created by Newell, Shaw, and Simon.
    \item \textbf{John McCarthy's Predicate Calculus:} John McCarthy introduced the idea of predicate calculus to represent knowledge through his program “Advice Taker,” allowing it to reason based on information instead of just following pre-written instructions.
    \item \textbf{Introduction of Lisp:} McCarthy defines the Lisp programming language in MIT AI Lab Memo No. 1, which becomes a dominant AI programming language for the next 30 years.
    \item \textbf{McCarthy’s "Programs with Common Sense":} McCarthy publishes a paper outlining the Advice Taker, considered one of the first complete AI systems designed to solve problems using knowledge. Notably, he introduced Lisp in MIT AI Lab Memo No. 1, establishing it as the primary programming language for AI development for the following three decades.
    \item \textbf{Marvin Minsky Joins MIT:} Minsky moves to MIT, collaborating with McCarthy but eventually diverging in focus from formal logic to practical programming.
    \item \textbf{Genetic Algorithms:} Friedberg introduces genetic algorithms, proposing that small mutations in machine-code programs can lead to improved performance, although early attempts show limited progress.
    \item \textbf{Perceptron Research:} Frank Rosenblatt's work on perceptrons garners attention; perceptrons are simple pattern recognition devices with basic learning capabilities based on linear threshold logic.
    \item \textbf{LISP Development Begins:} John McCarthy starts developing LISP, which becomes a prominent programming language in AI.
    \item \textbf{Formation of MIT AI Lab:} The Massachusetts Institute of Technology establishes its AI laboratory, fostering further AI research.
\end{itemize}


\subsection{Visual and Problem Solver (1959)}
\begin{itemize}

\item \textbf{Term Machine Learning:}
Arthur Samuel coined the term “machine learning” during a speech about teaching machines to play chess better than their human programmers.
    \item \textbf{Biological Vision Studies:} Research on biological vision systems informs AI development.
    \item \textbf{Geometry Theorem Prover:} Herbert Gelernter constructs a program capable of proving complex theorems.
    \item \textbf{General Problem Solver (GPS):} Newell, Shaw, and Simon develop the GPS, a significant AI program for problem-solving.
\end{itemize}

\subsection{Enhancements to Hebb’s Learning Methods (1960)}
 Bernie Widrow improves Hebb's methods with adalines, and Frank Rosenblatt develops perceptrons, advancing neural network research.

%% -------=================================-------=================================
% -------=================================-------=================================
% -------=================================-------=================================

\section{Excitement and Limitations (1960 - 1970)}
Early 1960s advancements in AI research involved continued exploration of pattern recognition and self-adapting systems as AI research evolved, building on foundations laid in the previous decade.

\subsection{Adaptive control theory (1960)}
\begin{itemize}
    \item Adaptive control theory was introduced by Widrow \& Hoff.
\end{itemize}

\subsection{Limitations and Industry (1961)}
\begin{itemize}
    \item Lucas argues that limitations in AI apply to both humans and machines.
    \item First Industrial Robot installed.
\end{itemize}

\subsection{Perceptron (1962)}
\begin{itemize}
    \item Frank Rosenblatt's work on perceptrons advances learning and pattern recognition.
    \item Bernie Widrow enhances Hebb's learning methods, calling his networks \textit{adalines}.
    \item Perceptron Convergence Theorem (Block et al., 1962): A perceptron's learning algorithm can adjust its connection strengths to fit any given input data, as long as a suitable match is possible.
\end{itemize}

\subsection{Vision and AI Labs (1963)}
\begin{itemize}
    \item Roberts contributed to significant advancements in machine vision, drawing insights from biological vision systems.
    \item John McCarthy established the AI lab at Stanford University.
    \item James Slagle's SAINT program effectively solved calculus integration problems typical of first-year college courses.
    \item Winograd and Cowan demonstrated that multiple elements can together represent individual concepts, enhancing robustness and parallelism.
\end{itemize}

\subsection{DENDRAL (1965)}
\begin{itemize}
    \item J. A. Robinson developed the resolution method for logical inference.
    \item At Stanford University, J. Lederberg, Edward Feigenbaum, and Carl Djerassi initiated the DENDRAL project.
    \item DENDRAL is an expert system that identifies molecular structures based solely on the compound's constituents and mass spectra data.
    \item DENDRAL is recognized as the first knowledge-based expert system ever created.
    \item ELIZA, a chatbot, was developed.
    \item Chomsky’s work in computational linguistics advanced the field.
\end{itemize}

\subsection{Funding Cut-off (1966)}
\begin{itemize}
    \item A report reveals the limitations of machine translation for scientific text, leading to the cancellation of U.S. government funding for academic translation projects.
    \item SYSTRAN, the first machine translation focused on Russian-English translation during the Cold War, was developed.
\end{itemize}

\subsection{Samuel's Checkers (1967)}
\begin{itemize}
    \item Samuel developed a program for playing checkers games.
    \item Daniel Bobrow's STUDENT program solved algebra story problems.
\end{itemize}

\subsection{Solve Mathematical Problems(1968)}
\begin{itemize}
    \item Tom Evans's ANALOGY program solved geometric analogy problems similar to those in IQ tests.
    \item The MACSYMA project was initiated at MIT by Carl Engelman, William Martin, and Joel Moses. This program, written in LISP, is a sophisticated interactive tool designed to address a variety of mathematical problems and builds upon the foundational work of the earlier SIN integration-solving program.
\end{itemize}

\subsection{Late 1960s}
\begin{itemize}
    \item \textbf{\large 1969:} 
    \begin{itemize}
        \item Minsky and Papert's book \textit{Perceptrons} demonstrated the limitations of single-layer perceptrons.
        \item Bryson and Ho discovered the back-propagation learning algorithms for multilayer networks.
        \item Cordell Green's QA3 system implemented predicate calculus for knowledge representation, establishing a foundation for AI in this area.
        \item The DENDRAL program, created by Feigenbaum, Buchanan, and Lederberg at Stanford, inferred molecular structures from mass spectrometer data.
    \end{itemize}
   
    \item \textbf{\large 1970:} 
    \begin{itemize}
        \item The introduction of the blocks world microworld paved the way for vision projects, including Patrick Winston's learning theory in 1970.
    \end{itemize}
\end{itemize}

% -------=================================-------=================================
% -------=================================-------=================================
% -------=================================-------=================================

\section{Knowledge-Based Systems (1970 - 1980)}
The late 1970s and early 1980s saw the rise of more sophisticated programs that incorporated extensive domain-specific knowledge, mimicking expert human performance in tasks like diagnosis and design.

\subsection{DENDRAL System (1971)}
\begin{itemize}
    \item DENDRAL, developed by Feigenbaum, Buchanan, and Lederberg, was one of the first systems to demonstrate the value of specialized knowledge, predicting organic molecule structures.
\end{itemize}

\subsection{MYCIN (1972)}
\begin{itemize}
    \item Developed by Feigenbaum, Buchanan, and Dr. Edward Shortliffe, MYCIN diagnosed blood infections using 450 rules, outperforming junior doctors.
\end{itemize}

\subsection{Lighthill Report (1973)}
\begin{itemize}
    \item The Lighthill report led to the British government ending support for AI research in all but two universities.
\end{itemize}

\subsection{Natural Language Understanding (1972 - 1973)}
\begin{itemize}
    \item Jerry Winograd's early natural language understanding system paved the way for continuous speech understanding projects, like NASA's LUNAR system, which could respond to spoken queries about lunar rock samples.
\end{itemize}

\subsection{First AI Winter (1973)}
\begin{itemize}
    \item The First AI Winter began due to disappointing results from early AI projects, leading to reduced funding.
\end{itemize}

\subsection{Blocks World (1974)}
\begin{itemize}
    \item The Blocks World was home to the vision project of Scott Fahlman, focused on planning.
\end{itemize}

\subsection{Objects, Events, and Frames (1975)}
\begin{itemize}
    \item David Waltz's vision and constraint-propagation work, along with Minsky's structured approach to organizing facts about objects and events into a taxonomic hierarchy.
\end{itemize}

\subsection{Shortliffe (1976)}
\begin{itemize}
    \item Expert systems for medical diagnosis emerged, leading to the formulation of the physical symbol system hypothesis by Newell and Simon.
\end{itemize}

\subsection{Schank (1977-1978)}
\begin{itemize}
    \item Schank and his students developed programs aimed at understanding natural language.
\end{itemize}

\subsection{Publication and Stanford Cart (1979)}
\begin{itemize}
    \item Pamela McCorduck's book, "Machines Who Think," provided a detailed history of AI, documenting its development, key figures, and philosophical questions. This year also saw the development of the Stanford Cart, an autonomous vehicle.
\end{itemize}


% -------=================================-------=================================
% -------=================================-------=================================
% -------=================================-------=================================



\section{Booming and AI Winter (1980-1990)}

In the mid-1980s, the back-propagation learning algorithm, originally discovered in 1969, was rediscovered by several groups and widely applied to learning problems, gaining attention through the publication of *Parallel Distributed Processing* in 1986. Connectionist models, supported by these algorithms, emerged as rivals to symbolic AI models and logic-based approaches. This period saw a resurgence in neural network research, leading to a significant expansion of the AI industry from millions in 1980 to billions by 1988. However, the "AI Winter" followed as many companies failed to meet high expectations.

Pascal’s work on calculating machines inspired Leibniz to develop the Leibniz Wheel in 1694, a machine capable of multiplication and division using a hand crank and moveable carriage. Fascinated by automated logic, Leibniz proposed a machine that could calculate logical conclusions based on the necessary and sufficient features of concepts. In 1887, he envisioned a device for automated deductive reasoning and scientific knowledge production, foreshadowing modern ideas of inference and proof automation.

\subsection{Competition and Efforts (1981)}
\begin{itemize}
    \item Bacon's algorithms were part of an AI program for discovery learning, inducing physical laws from data collections.
    \item In 1981, Japan launched the "Fifth Generation" project using Prolog, prompting the U.S. to form the MCC and Britain to restore AI funding through the Alvey report. 
    \item Despite efforts, none of these projects achieved their ambitious goals.
\end{itemize}

\subsection{Hopfield Network (1982)}
\begin{itemize}
    \item John Hopfield revived artificial neural networks (ANN) and introduced the Hopfield Network.
    \item Judea Pearl introduced normative expert systems that act rationally according to decision theory without imitating human thought processes.
    \item The first successful commercial expert system, R1, began operation at Digital Equipment Corporation, assisting in configuring orders for new computer systems.
    \item Other expert systems were developed for medical diagnosis and resource evaluation.
\end{itemize}

\subsection{Robot Control (1984)}
\begin{itemize}
    \item Efforts were made in mobile robot control, but scaling these programs to real-world problems revealed limitations, with many systems only solving "toy problems."
\end{itemize}

\subsection{BackPropagation Introduced (1986)}
\begin{itemize}
    \item By 1986, R1 saved Digital Equipment Corporation an estimated $40 million annually.
    \item Geoffrey Hinton introduced back-propagation, enhancing artificial neural networks.
    \item Eric Horvitz and David Heckerman promoted the idea of normative expert systems.
\end{itemize}

\subsection{Boom and AI Winter (1988)}
\begin{itemize}
    \item The AI industry grew to billions, with many companies developing expert systems, robots, and AI hardware.
    \item However, the "AI Winter" followed due to companies failing to meet their promises, despite DEC using 40 expert systems and DuPont utilizing 100, saving $10 million annually.
\end{itemize}

\subsection{Probabilistic Reasoning and Data Mining (1988)}
\begin{itemize}
    \item Data mining fueled a new industry, while Judea Pearl's *Probabilistic Reasoning in Intelligent Systems* revived probability and decision theory in AI, building on interest sparked by Peter Cheeseman's 1985 article, "In Defense of Probability."
\end{itemize}

\subsection{Limitations (1989-1990)}
\begin{itemize}
    \item By 1989, Penrose and most logicians believed that limitations apply to both humans and machines.
\end{itemize}


% -------=================================-------=================================
% -------=================================-------=================================
% -------=================================-------=================================

\section{Modern AI and Machine Learning (1991 - 2000)}

In this era, machine translation evolved similarly to speech recognition, experiencing initial enthusiasm in the 1950s for word sequence-based models grounded in information theory. This approach fell out of favor in the 1960s but made a resurgence in the late 1990s, now dominating the field of machine translation.

Bayesian networks efficiently represent and reason with uncertain knowledge, addressing earlier probabilistic systems' limitations. They became central to AI research in uncertain reasoning and expert systems, supporting learning from experience and integrating classical AI with neural networks.

Allen Newell, John Laird, and Paul Rosenbloom created SOAR, a well-known complete agent architecture. The Internet became a crucial environment for intelligent agents, making AI systems common in web applications and popularizing the “-bot” suffix in everyday language.

Sensory systems like vision and speech recognition often lack reliability, requiring reasoning and planning systems to manage uncertainty effectively. AI increasingly connected with fields like control theory and economics, focusing on agents. Recent advances in robotic car control resulted from improved sensors and the integration of sensing, localization, mapping, and high-level planning.

\subsection{CYC Project (1995)}
Natural language understanding systems were limited to specific domains, necessitating improvements in general knowledge representation. The CYC project sought to compile commonsense knowledge. Methodologically, AI firmly embraced the scientific method, requiring hypotheses to undergo rigorous empirical experiments, with results analyzed statistically (Cohen, 1995).

\subsection{AI Victory over Human Chess Champion (1997)}
AI made significant strides in game-playing, culminating in IBM's DEEP BLUE's victory over world chess champion Garry Kasparov in 1997, showcasing advanced search algorithms and specialized hardware.

\subsection{Breaking Isolation (1998)}
David McAllester (1998) noted that early AI relied on symbolic computation, leading to isolation from mainstream computer science. Recognition grew that AI should integrate with machine learning, information theory, uncertain reasoning, stochastic modeling, search and optimization, and automated reasoning with formal methods.

\section{Data Explosion \& Neural Networks (2000 - 2010)}

Historically, computer science focused on algorithms. Recent AI research indicates that prioritizing data over specific algorithms is often more beneficial. This shift is driven by vast data sources, including trillions of words, billions of images (Kilgarriff and Grefenstette, 2006), and genomic sequences (Collins et al., 2003).

\subsection{2006}
Geoffrey Hinton popularized the term “Deep Learning” and Multilayer Artificial Neural Networks.

\subsection{AGI Conference (2007-2008)}
AI tools powered devices like Apple Siri, enhancing NLP capabilities. Artificial General Intelligence (AGI) (Goertzel and Pennachin, 2007) aimed to develop a universal algorithm capable of learning and operating in any environment, originating from Ray Solomonoff's 1964 work. AGI's first conference and the Journal of Artificial General Intelligence were established in 2008, with a key concern ensuring that created AI systems are genuinely Friendly AI (Yudkowsky, 2008; Omohundro, 2008).

\subsection{Discontent with AI Progress (2005, 2007, 2009)}
Despite significant achievements, figures like John McCarthy (2007), Marvin Minsky (2007), Nils Nilsson (1995, 2005), and Patrick Winston (Beal and Winston, 2009) expressed dissatisfaction with AI advancements. They advocated focusing less on advanced applications for specific tasks and more on creating “machines that think, that learn, and that create,” a vision articulated by Simon. This initiative is known as human-level AI (HLAI), with its first symposium held in 2004 (Minsky et al., 2004).

\section{AI Revolutions and Deep Learning (2010 - 2020)}

\subsection{2012}
AlexNet, a CNN, won the ImageNet Competition, marking a groundbreaking work in computer vision.

\subsection{2014}
Google's AlphaGo defeated a professional Go player.

\subsection{2015}
OpenAI was founded.

\subsection{OpenAI Revolutionized: AI Hype-Race, Building AGI? (2020 - Present)}
\begin{itemize}
    \item  \textbf {2021:} ChatGPT-3, First LLMs with over 174 billion parameters (revolution in AI)ChatGPT-3, with an unprecedented ability to generate human-like text. This advancement revolutionized natural language processing, enabling applications from chatbots to content creation tools.
    \item  \textbf {2022:} DALL-E and Stable Diffusion Models emerged as generative AI.
    \item  \textbf {2023-Present:} Different versions of ChatGPT with multi-modal features like text-to-image, text-to-voice, and speech recognition integration of different models in a single LLMs via software engineering not an AGI model. \\
    Ongoing work to make AGI true by OpenAI and Google DeepMind’s Project
\end{itemize}

% -------=================================-------=================================
% -------=================================-------=================================
% -------=================================-------=================================

\section{Historical Events and Facts}
Either philosophical, mechanical, or digital. There were many historical attempts to make machines intelligent. In the ancient era philosophers and critical thinkers have given thought and tried to decipher the intelligent machine. "machine intelligence" "cognology", and "artificial intelligence” are the proposed names for the field. We would be calling machine intelligence instead of Artificial intelligence if the AI term is not chosen and accepted. Twentieth-century logicians like Kurt Gödel, Stephen Kleene, and Alan Turing clarified the limits of logical and computational systems. While some philosophers viewed these limitations as evidence that human intelligence could not be mechanized. most logicians and computer scientists believe that these limitations apply to both humans and machines.

AI research captured significant attention on human intelligence's various capabilities including visual perception and language comprehension. It was predicted that there would likely be a greater focus on integrated autonomous systems—robots and "softbots. " Softbots, or software agents, navigate the Internet to discover relevant information for users. The continuous drive to enhance the capabilities of robotic and software agents will shape the future of AI research for years to come. Now at the present 2024, we can see chatbots, LLMs, customer service, and virtual assistants acting as intelligent agents.

John McCarthy's moving to Dartmouth from Stanford in 1951 ignited the formal birth of AI as a distinct field that attempts to build machines that will function autonomously in complex and changing environments. AI was founded in part as a rebellion against the limitations of existing fields like control theory and statistics, but now it is embracing those fields.

Period of 1952-1969 John McCarthy referred to this period as the “Look, Ma, no hands!” era because it was an early success where every initial challenge was solved. New advancements in formal grammar emerged from early 1900s logic, offering fresh approaches to language theories within the broader field of linguistics.

In the 1950s, the electronic stored-program digital computer became commercially viable, following prototypes like the Mark I, ENIAC, EDVAC, and UNIVAC. Key developments that contributed to the launch of AI included Claude Shannon's information theory, neurological models from psychologists, and advancements in Boolean algebra, switching theory, and statistical decision theory. Importantly, AI's foundation was built on centuries of research from figures such as Aristotle, Leibnitz, Babbage, and Hollerith, whose contributions paved the way for modern AI.
The late 1950s saw the rise of significant AI projects such as GPS: A general problem-solver, a Geometry Theorem-Proving Machine: Focused on geometric theorems(Developed proofs from basic axioms as a sequence of subgoals), and EPAM: For pattern recognition and memory(Learned stimulus-response pairs through repetitive presentations and Demonstrated the learned responses to stimuli).
Herbert Simon's 1957 statement is frequently cited:
"I don’t intend to surprise you, but simply put, there are now machines that can think, learn, and create. Furthermore, their capabilities will grow swiftly, and in the foreseeable future, the range of problems they can solve will match that of the human mind."
An illustrative incident from early machine translation efforts involved significant funding from the U.S. National Research Council aimed at accelerating the translation of Russian scientific papers following the Sputnik launch in 1957. However, the task proved challenging. A well-known mistranslation, where "the spirit is willing but the flesh is weak" was incorrectly rendered as "the vodka is good but the meat is rotten," highlights the difficulties faced in these initial attempts.
In the 1960s and early 1970s, AI research concentrated on problem representation, search techniques, and general heuristics to develop computer programs that could solve puzzles, play games, and retrieve information.
From 1966 to 1973, AI researchers, including Herbert Simon, confidently predicted rapid advancements in machine capabilities. Simon famously stated in 1957 that machines could think, learn, and create, and he expected them to tackle problems as effectively as the human mind within a "visible future." He anticipated that a computer would become a chess champion and prove significant mathematical theorems within ten years. While some of these predictions eventually came true, it took around 40 years instead of the projected ten, as early AI systems often struggled with more complex challenges beyond simple tasks.
The first AI Winter is generally considered to have occurred in the late 1960s and early 1970s, following overblown expectations and limited successes in early AI research.
A second AI Winter took place in the 1980s, partly due to the high cost of early AI hardware and software, as well as a lack of practical applications.

AI in the 1990s, this trend has reversed in recent years as tools from machine learning in particular have proved effective for many problems. The process of reintegration is already yielding significant benefits.


The first \textbf{AI Winter} is generally considered to have occurred in the \textit{late 1960s and early 1970s}, following \textbf{overblown expectations} and \textit{limited successes} in early AI research.\\
A second \textbf{AI Winter} took place in the \textit{1980s}, partly due to the \textbf{high cost} of early AI hardware and software, as well as a \textit{lack of practical applications}.\\

% -------=================================-------=================================
% -------=================================-------=================================
% -------=================================-------=================================

\section{Limitations and Discussion}

While systems like Expert Systems, DENDRAL, and ELIZA were significant advancements in their time, they were limited by their reliance on explicit programming and rule-based approaches. However, recent advancements in machine learning, particularly deep learning, have shown promising results in addressing these limitations. Neural networks, inspired by the structure of the human brain, have demonstrated remarkable capabilities in tasks such as image recognition, natural language processing, computer vision, creativity/generative, translation, code generation, analysis, and many more.

Many problems once perceived as hard to solve have become easier, while those thought to be simple have grown more complex. This shift highlights the importance of not underestimating challenges, as our understanding evolves. We may not fully grasp all the components needed to create true AI, so it's essential to be guided by mathematics, research, experiments, and insights from various interdisciplinary fields.

In reviewing historical innovations and their implementations, it becomes evident that substantial computing power and resources will be requisite. As technological advancements progress increasingly, the demand for such capabilities escalates correspondingly. However, there will always be dedicated stakeholders, entities, or agents actively engaged in the pursuit of enhancing efficiency and optimization.

The history of artificial intelligence (AI) is marked by a series of significant events and advancements, as well as notable failures within the field. Given the vastness of the subject, this review paper aims to catalog these milestones, emphasizing their importance rather than delving into extensive critical analysis.

It is important to note that until now [2024] no machine has surpassed the Turing Test and human intelligence, although many argue ultimately, machines will surpass human intelligence. Also, nobody has a clear pathway to developing Artificial General Intelligence (AGI).  There is no set timeline for AGI’s practical realization, and it is still a topic of debate.

Effective leadership and motivation, as exemplified by the Dartmouth Conference, are essential for progress. An open approach, rather than criticism, encourages the exploration of diverse ideas, which can enhance artificial intelligence's ability to innovate and think creatively.

\section{Conclusion}
The history and foundations of AI illustrate a rich and evolving field influenced by both theoretical advancements and practical applications. By examining the contributions of key figures and the shifting paradigms within AI, we gain a deeper understanding of its development into its current form. 

For instance, while symbolic rules dominated the field in the past year, emerging connectionist ideas are now taking precedence. However, this shift does not imply that previous works in AI are irrelevant; rather, we cannot predict when certain mathematics, concepts, ideas, or research will become necessary in the future. Both symbol-based computation and connectionist approaches are significant, complementary, and subject to debate.

The recent hype surrounding AI and the competition among major tech companies signals progress toward the development of true AI or Artificial General Intelligence (AGI). Therefore, the trajectory of this field is unpredictable, and the ideas and rules we currently employ may or may not remain sufficient or relevant in the future. Transformations, financial wealth generated by AI, democratizing AI, guidelines and proper rules, and ethics in AI are some parameters to consider!
\begin{thebibliography}{9}
\bibitem{poole} 
David L. Poole and Alan K. Mackworth. 
\textit{Artificial Intelligence: Foundations of Computational Agents}. Cambridge University Press, 2010.

\bibitem{nilsson} 
Nils J. Nilsson. 
\textit{The Quest for Artificial Intelligence: A History of Ideas and Achievements}. Cambridge University Press, 2010.

\bibitem{russell} 
Stuart Russell and Peter Norvig. 
\textit{Artificial Intelligence: A Modern Approach}. Pearson, 2016.

\bibitem{patterson} 
Daniel W. Patterson. 
\textit{Artificial Intelligence and Expert Systems}. Prentice Hall, 1990.

\bibitem{luger} 
George F. Luger. 
\textit{Artificial Intelligence: Structures and Strategies for Complex Problem Solving}. Benjamin/Cummings Publishing, 2009.

\bibitem{tableau} 
Tableau. 
\textit{History of AI}. Available at:\url{https://www.tableau.com/data-insights/ai/history}


\end{thebibliography}

\end{document}

